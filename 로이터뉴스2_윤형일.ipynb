{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "로이터뉴스2-윤형일.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yg9HrKDG16Bz",
        "NkqlT2dVKmov",
        "0MryS968OM7I",
        "Ozt3ODMPWOpw",
        "dQT0-6CCh3YR",
        "2W1jqaM3j-JE",
        "2r9HCPoGk5SB"
      ],
      "authorship_tag": "ABX9TyP/FdyFv1AjXakyuNnAn4cS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunhyeongil/pandas_base/blob/main/%EB%A1%9C%EC%9D%B4%ED%84%B0%EB%89%B4%EC%8A%A42_%EC%9C%A4%ED%98%95%EC%9D%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg9HrKDG16Bz"
      },
      "source": [
        "# Sequential 신경망"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knHPC46o15ha"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0PZZ6rV1yUZ"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xFxojwQ2oOv",
        "outputId": "d9b613f6-bdbe-4d7b-8c00-6c62fa97935d"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=1000, test_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:144: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZxk0UXz2u4a",
        "outputId": "c3996fbf-fa37-4a0b-ef9b-eea801d7d2c1"
      },
      "source": [
        "print('훈련용 뉴스 기사 : {}'.format(len(X_train)))\n",
        "print('테스트용 뉴스 기사 : {}'.format(len(X_test)))\n",
        "num_classes = max(y_train) + 1\n",
        "print('카테고리 : {}'.format(num_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련용 뉴스 기사 : 8982\n",
            "테스트용 뉴스 기사 : 2246\n",
            "카테고리 : 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gDkSDxK253L",
        "outputId": "c597b353-2f29-4050-c15d-9a1adb13281c"
      },
      "source": [
        "print(X_train[0]) # 첫번째 훈련용 뉴스 기사\n",
        "print(y_train[0]) # 첫번째 훈련용 뉴스 기사의 레이블\n",
        "print(len(X_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "3\n",
            "87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJst1UPm6CjM"
      },
      "source": [
        "max_len = max(len(l) for l in X_train)\n",
        "max_len = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em7UdXLC28yv"
      },
      "source": [
        "max_len = 100\n",
        "X_train = pad_sequences(X_train, maxlen=max_len) # 훈련용 뉴스 기사 패딩\n",
        "X_test = pad_sequences(X_test, maxlen=max_len) # 테스트용 뉴스 기사 패딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVVYAHLt5BJH",
        "outputId": "ecc18a16-36cd-499f-920b-314060a1dca3"
      },
      "source": [
        "print(X_train[0])\n",
        "print(len(X_train))\n",
        "print(len(X_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   2   8  43\n",
            "  10 447   5  25 207 270   5   2 111  16 369 186  90  67   7  89   5  19\n",
            " 102   6  19 124  15  90  67  84  22 482  26   7  48   4  49   8 864  39\n",
            " 209 154   6 151   6  83  11  15  22 155  11  15   7  48   9   2   2 504\n",
            "   6 258   6 272  11  15  22 134  44  11  15  16   8 197   2  90  67  52\n",
            "  29 209  30  32 132   6 109  15  17  12]\n",
            "8982\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWUDGzlW5HNo",
        "outputId": "05b3f1ca-7a04-40e5-c5d6-8f102ecb94ef"
      },
      "source": [
        "print(y_train)\n",
        "print(len(y_train[0]))\n",
        "print(len(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "46\n",
            "8982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AIa1_NN5UUU"
      },
      "source": [
        "y_train = to_categorical(y_train) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
        "y_test = to_categorical(y_test) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tGQj5Dm5Yvl",
        "outputId": "b997f913-d8d5-44b5-afdd-976d4dafdb7f"
      },
      "source": [
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmb0KsdZ8i9D",
        "outputId": "dd30db17-52fb-4617-ea81-869b286ebac9"
      },
      "source": [
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS30-3_zD8sP",
        "outputId": "e19a2211-2339-410e-a8ec-66634b38e767"
      },
      "source": [
        "print(len(word_index.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLLa6HXQ8o8b"
      },
      "source": [
        "index_to_word = {index +3 : word for word, index in word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nE9Beee8xgD"
      },
      "source": [
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  index_to_word[index] = token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pquSBKa6VDr"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index.keys()),200, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(46, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UySr_9pG-BD4",
        "outputId": "df8bc7c7-82ac-48a6-9dbe-cc309b31ffdc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 4)            123916    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 46)                18446     \n",
            "=================================================================\n",
            "Total params: 142,362\n",
            "Trainable params: 142,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Qzpb6M-ESl",
        "outputId": "cfd43e79-1f03-49a0-f25a-a2f38bdf8084"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 훈련 조기 종료\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 훈련 과정에서 검증 데이터가 가장 높았을 때 저장\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train,  batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - 7s 96ms/step - loss: 0.2165 - acc: 0.9454 - val_loss: 1.4629 - val_acc: 0.6812\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.68121, saving model to best_model.h5\n",
            "Epoch 2/30\n",
            "71/71 [==============================] - 7s 94ms/step - loss: 0.1482 - acc: 0.9535 - val_loss: 1.5080 - val_acc: 0.6808\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.68121\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - 7s 94ms/step - loss: 0.1414 - acc: 0.9530 - val_loss: 1.5680 - val_acc: 0.6772\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.68121\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - 7s 93ms/step - loss: 0.1348 - acc: 0.9555 - val_loss: 1.5399 - val_acc: 0.6870\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.68121 to 0.68700, saving model to best_model.h5\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - 7s 94ms/step - loss: 0.1382 - acc: 0.9521 - val_loss: 1.5893 - val_acc: 0.6808\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.68700\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wyJ52wG-IZc",
        "outputId": "cbc5166f-e1c6-4645-9e86-a69221c89d1f"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 0s 3ms/step - loss: 1.5399 - acc: 0.6870\n",
            "\n",
            " 테스트 정확도: 0.6870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b8Q3fF7U1YH"
      },
      "source": [
        "# 결과 0.6870"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkqlT2dVKmov"
      },
      "source": [
        "# RNN의 일종인  LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnASY5ZRHaud"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(1000, 120))\n",
        "model.add(LSTM(120))\n",
        "model.add(Dense(46, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGkUCjktIk1K"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uG3n08OK0nl"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjjTkaIlK3Cl",
        "outputId": "f57ebc86-56ba-495e-9a07-123fb6f1d9a3"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - 26s 345ms/step - loss: 2.6356 - acc: 0.3387 - val_loss: 2.4019 - val_acc: 0.3620\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.36198, saving model to best_model.h5\n",
            "Epoch 2/30\n",
            "71/71 [==============================] - 24s 339ms/step - loss: 2.1429 - acc: 0.4371 - val_loss: 1.9664 - val_acc: 0.5022\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.36198 to 0.50223, saving model to best_model.h5\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - 24s 338ms/step - loss: 1.8895 - acc: 0.5219 - val_loss: 1.8051 - val_acc: 0.5530\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.50223 to 0.55298, saving model to best_model.h5\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - 24s 338ms/step - loss: 1.7474 - acc: 0.5481 - val_loss: 1.7157 - val_acc: 0.5481\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.55298\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - 24s 344ms/step - loss: 1.6815 - acc: 0.5662 - val_loss: 1.6878 - val_acc: 0.5721\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.55298 to 0.57213, saving model to best_model.h5\n",
            "Epoch 6/30\n",
            "71/71 [==============================] - 24s 345ms/step - loss: 1.6303 - acc: 0.5825 - val_loss: 1.6737 - val_acc: 0.5824\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.57213 to 0.58237, saving model to best_model.h5\n",
            "Epoch 7/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 1.6086 - acc: 0.5843 - val_loss: 1.6067 - val_acc: 0.6011\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.58237 to 0.60107, saving model to best_model.h5\n",
            "Epoch 8/30\n",
            "71/71 [==============================] - 24s 344ms/step - loss: 1.5255 - acc: 0.6168 - val_loss: 1.5408 - val_acc: 0.6162\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.60107 to 0.61621, saving model to best_model.h5\n",
            "Epoch 9/30\n",
            "71/71 [==============================] - 24s 340ms/step - loss: 1.4594 - acc: 0.6391 - val_loss: 1.4843 - val_acc: 0.6389\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.61621 to 0.63891, saving model to best_model.h5\n",
            "Epoch 10/30\n",
            "71/71 [==============================] - 24s 341ms/step - loss: 1.3533 - acc: 0.6615 - val_loss: 1.5306 - val_acc: 0.6109\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.63891\n",
            "Epoch 11/30\n",
            "71/71 [==============================] - 24s 341ms/step - loss: 1.2817 - acc: 0.6818 - val_loss: 1.4032 - val_acc: 0.6407\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.63891 to 0.64069, saving model to best_model.h5\n",
            "Epoch 12/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 1.2214 - acc: 0.6946 - val_loss: 1.3686 - val_acc: 0.6527\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.64069 to 0.65272, saving model to best_model.h5\n",
            "Epoch 13/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 1.1622 - acc: 0.7051 - val_loss: 1.3608 - val_acc: 0.6603\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.65272 to 0.66028, saving model to best_model.h5\n",
            "Epoch 14/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 1.1130 - acc: 0.7183 - val_loss: 1.3293 - val_acc: 0.6603\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.66028\n",
            "Epoch 15/30\n",
            "71/71 [==============================] - 24s 345ms/step - loss: 1.0623 - acc: 0.7310 - val_loss: 1.3081 - val_acc: 0.6768\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.66028 to 0.67676, saving model to best_model.h5\n",
            "Epoch 16/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 1.0137 - acc: 0.7413 - val_loss: 1.3120 - val_acc: 0.6745\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.67676\n",
            "Epoch 17/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 0.9893 - acc: 0.7438 - val_loss: 1.2799 - val_acc: 0.6736\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.67676\n",
            "Epoch 18/30\n",
            "71/71 [==============================] - 24s 339ms/step - loss: 0.9966 - acc: 0.7444 - val_loss: 1.3277 - val_acc: 0.6679\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.67676\n",
            "Epoch 19/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 0.9357 - acc: 0.7609 - val_loss: 1.2906 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.67676 to 0.68566, saving model to best_model.h5\n",
            "Epoch 20/30\n",
            "71/71 [==============================] - 24s 340ms/step - loss: 0.8844 - acc: 0.7727 - val_loss: 1.2615 - val_acc: 0.6870\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.68566 to 0.68700, saving model to best_model.h5\n",
            "Epoch 21/30\n",
            "71/71 [==============================] - 24s 339ms/step - loss: 0.8405 - acc: 0.7876 - val_loss: 1.2573 - val_acc: 0.7026\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.68700 to 0.70258, saving model to best_model.h5\n",
            "Epoch 22/30\n",
            "71/71 [==============================] - 24s 338ms/step - loss: 0.8132 - acc: 0.7927 - val_loss: 1.2832 - val_acc: 0.6981\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.70258\n",
            "Epoch 23/30\n",
            "71/71 [==============================] - 24s 339ms/step - loss: 0.8079 - acc: 0.7928 - val_loss: 1.2624 - val_acc: 0.7004\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.70258\n",
            "Epoch 24/30\n",
            "71/71 [==============================] - 24s 341ms/step - loss: 0.7463 - acc: 0.8115 - val_loss: 1.2746 - val_acc: 0.7030\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.70258 to 0.70303, saving model to best_model.h5\n",
            "Epoch 25/30\n",
            "71/71 [==============================] - 24s 339ms/step - loss: 0.7148 - acc: 0.8161 - val_loss: 1.2732 - val_acc: 0.6990\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.70303\n",
            "Epoch 00025: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xak5syLOK5sI",
        "outputId": "f22ed8f5-85b0-4234-ed03-b2318c694269"
      },
      "source": [
        "ded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 0s 3ms/step - loss: 1.5399 - acc: 0.6870\n",
            "\n",
            " 테스트 정확도: 0.6870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRb14-OvUet4"
      },
      "source": [
        "# 결과 :  0.6870"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MryS968OM7I"
      },
      "source": [
        "# 양방향 LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VBNY6sxNQN-"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPq1Opw2OTgS",
        "outputId": "2089228a-3bef-4ecb-d293-3f38f964198f"
      },
      "source": [
        "max_words = 1000\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:144: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgzaW4OGOVgg",
        "outputId": "8ffafa36-e5d3-4cad-b46b-690eea5e2080"
      },
      "source": [
        "print(f'훈련 샘플의 수 : {len(x_train)}')\n",
        "print(f'테스트 샘플의 수 : {len(x_test)}')\n",
        "num_classes = max(y_train) + 1\n",
        "print(f'클래스의 수 : {num_classes}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플의 수 : 8982\n",
            "테스트 샘플의 수 : 2246\n",
            "클래스의 수 : 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQZexr-P95Z"
      },
      "source": [
        "pad_sequences()<br>\n",
        "keras.preprocessing.sequence.pad_sequences<br>(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvda22ZMObAp"
      },
      "source": [
        "max_len = 100\n",
        "x_train = pad_sequences(x_train, maxlen=max_len) # 훈련용 뉴스 기사 패딩\n",
        "x_test = pad_sequences(x_test, maxlen=max_len) # 테스트용 뉴스 기사 패딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyjQQwI6Osli",
        "outputId": "0136e3b4-4575-4000-df92-da926399fcb2"
      },
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (8982, 100)\n",
            "x_test shape: (2246, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JALdNq3lOutR"
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
        "y_test = to_categorical(y_test, num_classes) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcTOwz43Ozd1",
        "outputId": "68c68795-96d2-4de2-ffef-a8095b0556ee"
      },
      "source": [
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htKoPfYGQ_E3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUkCZ04WQoBh"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_ik_zi0O1RF"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juQGWpM0QjfS"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 훈련 조기 종료\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 훈련 과정에서 검증 데이터가 가장 높았을 때 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yp73fbVR1bh"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ur9DH54R5-_",
        "outputId": "bb4d355c-5be2-479a-b511-0e78aa81f6d5"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - 28s 352ms/step - loss: 2.6372 - acc: 0.3378 - val_loss: 2.1319 - val_acc: 0.3744\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.37444, saving model to best_model.h5\n",
            "Epoch 2/30\n",
            "71/71 [==============================] - 24s 345ms/step - loss: 1.9918 - acc: 0.4738 - val_loss: 1.8340 - val_acc: 0.5062\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.37444 to 0.50623, saving model to best_model.h5\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - 25s 345ms/step - loss: 1.7524 - acc: 0.5411 - val_loss: 1.6752 - val_acc: 0.5708\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.50623 to 0.57079, saving model to best_model.h5\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - 24s 345ms/step - loss: 1.6868 - acc: 0.5599 - val_loss: 1.6544 - val_acc: 0.5699\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.57079\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - 24s 341ms/step - loss: 1.5624 - acc: 0.5943 - val_loss: 1.5479 - val_acc: 0.6020\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.57079 to 0.60196, saving model to best_model.h5\n",
            "Epoch 6/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 1.4710 - acc: 0.6150 - val_loss: 1.5445 - val_acc: 0.6126\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.60196 to 0.61264, saving model to best_model.h5\n",
            "Epoch 7/30\n",
            "71/71 [==============================] - 24s 345ms/step - loss: 1.4339 - acc: 0.6230 - val_loss: 1.4730 - val_acc: 0.6269\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.61264 to 0.62689, saving model to best_model.h5\n",
            "Epoch 8/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 1.3586 - acc: 0.6475 - val_loss: 1.4767 - val_acc: 0.6282\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.62689 to 0.62823, saving model to best_model.h5\n",
            "Epoch 9/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 1.4284 - acc: 0.6220 - val_loss: 1.5099 - val_acc: 0.6113\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.62823\n",
            "Epoch 10/30\n",
            "71/71 [==============================] - 24s 341ms/step - loss: 1.3137 - acc: 0.6565 - val_loss: 1.4211 - val_acc: 0.6398\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.62823 to 0.63980, saving model to best_model.h5\n",
            "Epoch 11/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 1.2379 - acc: 0.6826 - val_loss: 1.3787 - val_acc: 0.6549\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.63980 to 0.65494, saving model to best_model.h5\n",
            "Epoch 12/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 1.1989 - acc: 0.6977 - val_loss: 1.3511 - val_acc: 0.6665\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.65494 to 0.66652, saving model to best_model.h5\n",
            "Epoch 13/30\n",
            "71/71 [==============================] - 24s 344ms/step - loss: 1.1435 - acc: 0.7165 - val_loss: 1.3524 - val_acc: 0.6719\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.66652 to 0.67186, saving model to best_model.h5\n",
            "Epoch 14/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 1.1168 - acc: 0.7188 - val_loss: 1.3445 - val_acc: 0.6714\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.67186\n",
            "Epoch 15/30\n",
            "71/71 [==============================] - 24s 341ms/step - loss: 1.0809 - acc: 0.7257 - val_loss: 1.3192 - val_acc: 0.6745\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.67186 to 0.67453, saving model to best_model.h5\n",
            "Epoch 16/30\n",
            "71/71 [==============================] - 24s 344ms/step - loss: 1.0510 - acc: 0.7383 - val_loss: 1.3024 - val_acc: 0.6817\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.67453 to 0.68166, saving model to best_model.h5\n",
            "Epoch 17/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 1.0139 - acc: 0.7484 - val_loss: 1.2983 - val_acc: 0.6937\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.68166 to 0.69368, saving model to best_model.h5\n",
            "Epoch 18/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 0.9854 - acc: 0.7547 - val_loss: 1.2814 - val_acc: 0.6937\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.69368\n",
            "Epoch 19/30\n",
            "71/71 [==============================] - 24s 344ms/step - loss: 0.9767 - acc: 0.7558 - val_loss: 1.2906 - val_acc: 0.6799\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.69368\n",
            "Epoch 20/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 0.9422 - acc: 0.7689 - val_loss: 1.2966 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.69368\n",
            "Epoch 21/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 0.9449 - acc: 0.7643 - val_loss: 1.2978 - val_acc: 0.6874\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.69368\n",
            "Epoch 22/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 0.9012 - acc: 0.7725 - val_loss: 1.2813 - val_acc: 0.6999\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.69368 to 0.69991, saving model to best_model.h5\n",
            "Epoch 23/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 0.8827 - acc: 0.7831 - val_loss: 1.2849 - val_acc: 0.7012\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.69991 to 0.70125, saving model to best_model.h5\n",
            "Epoch 24/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 0.8651 - acc: 0.7851 - val_loss: 1.3001 - val_acc: 0.6919\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.70125\n",
            "Epoch 25/30\n",
            "71/71 [==============================] - 24s 342ms/step - loss: 0.8575 - acc: 0.7879 - val_loss: 1.3182 - val_acc: 0.6972\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.70125\n",
            "Epoch 26/30\n",
            "71/71 [==============================] - 24s 343ms/step - loss: 0.8263 - acc: 0.7974 - val_loss: 1.2920 - val_acc: 0.6995\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.70125\n",
            "Epoch 00026: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3et_2yMR8UF",
        "outputId": "88b092f8-c925-4674-c7d7-80a260386ecb"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 3s 27ms/step - loss: 1.2849 - acc: 0.7012\n",
            "\n",
            " 테스트 정확도: 0.7012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAFyOynWVi7Y"
      },
      "source": [
        "# 결과 :  0.7012"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozt3ODMPWOpw"
      },
      "source": [
        "# TF-IDF-ADM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeCDSp2LWFBR"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zQsbwttWl3v",
        "outputId": "ec32e13b-e018-4507-c671-6682222ed624"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:144: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-MCPpqmWnjl"
      },
      "source": [
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc4AIgQiWxKn",
        "outputId": "388b2692-2a26-4e2b-d966-10833de6d068"
      },
      "source": [
        "print(f'훈련 샘플의 수 : {len(x_train)}') # train sequences\n",
        "print(f'테스트 샘플의 수 : {len(x_test)}') # test sequences\n",
        "num_classes = max(y_train) + 1\n",
        "print(f'클래스의 수 : {num_classes}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플의 수 : 8982\n",
            "테스트 샘플의 수 : 2246\n",
            "클래스의 수 : 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnWwNYvFW0Hw"
      },
      "source": [
        "index_to_word = {}\n",
        "for key, value in word_index.items():\n",
        "    index_to_word[value] = key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWr7Hi9tW58n",
        "outputId": "a0a0c38c-793a-46fd-fc9c-53e53616637a"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(x_train[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8982,)\n",
            "(2246,)\n",
            "(8982,)\n",
            "(2246,)\n",
            "[list([1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])\n",
            " list([1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 30625, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12])\n",
            " list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 11733, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 13051, 71, 5, 160, 63, 11, 9, 26503, 81, 5, 102, 59, 11, 17, 12])\n",
            " list([1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 465, 893, 3541, 114, 2902, 69, 312, 35, 15, 7, 335, 1679, 21, 25, 3675, 19519, 3498, 58, 69, 68, 493, 5, 25, 465, 377, 2430, 4, 293, 1172, 739, 4379, 8, 7, 1510, 1131, 13, 899, 6, 4, 990, 309, 415, 4519, 6920, 645, 3916, 791, 5, 4379, 75, 8, 24, 10, 1311, 4677, 5, 344, 756, 7, 29700, 231, 9691, 2603, 1413, 43, 509, 43, 68, 327, 5, 14560, 3498, 297, 638, 73, 430, 22, 4, 580, 7, 48, 41, 30, 14021, 136, 4, 344, 298, 4, 580, 40, 344, 5078, 23457, 291, 1488, 10, 3148, 5, 231, 6250, 1308, 5, 8250, 7043, 21, 18776, 1622, 990, 309, 415, 265, 5992, 8945, 1149, 9118, 27677, 4, 344, 9691, 756, 3729, 14560, 4667, 28400, 3249, 28, 10, 2190, 24, 77, 41, 682, 10, 4851, 2048, 7, 4, 5540, 2926, 1598, 22, 370, 5954, 7541, 5, 54, 5232, 1685, 2916, 10, 1571, 946, 60, 51, 3249, 5249, 4, 73, 2135, 669, 4, 580, 64, 10, 4280, 6, 16319, 25, 482, 35, 150, 377, 2430, 7, 10, 21743, 836, 29981, 4730, 6920, 5, 4379, 12711, 16799, 3541, 8, 4, 344, 291, 29693, 298, 4228, 6, 2223, 24, 14560, 41, 343, 430, 210, 6, 3498, 297, 64, 10, 2281, 455, 5, 7003, 125, 222, 17, 12])\n",
            " list([1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 22, 265, 9, 621, 575, 1080, 4742, 1149, 15874, 6, 438, 8295, 13, 102, 388, 15, 90, 67, 7, 197, 8295, 8, 4, 270, 416, 23, 527, 6, 15874, 4891, 4, 1055, 742, 16, 8, 36, 1480, 6, 2124, 100, 543, 5, 645, 362, 6, 2912, 4, 49, 8, 15874, 976, 124, 20, 5, 8295, 80, 9, 100, 362, 543, 395, 61, 44, 20, 8295, 8, 16, 40, 1276, 42, 1436, 166, 415, 6, 888, 4, 116, 9, 40, 3089, 4, 303, 163, 16, 64, 772, 13, 94, 156, 17, 12])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfu-d5YOW7jS",
        "outputId": "a15b4fc8-271f-4d60-8235-b021e49524af"
      },
      "source": [
        "print(' '.join([index_to_word[x] for x in x_train[0]]))\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0MSZzG8X8Ig"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfWI3Wn9fdk6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "test_seq = [[1,2,3,4,5,6]]\n",
        "\n",
        "tok = Tokenizer(num_words=10)\n",
        "tok.fit_on_sequences(test_seq)\n",
        "\n",
        "tok.sequences_to_matrix(test_seq)\n",
        "array([[0., 1., 1., 1., 1., 1., 1., 0., 0., 0.]])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfSe_Ov5XOCt",
        "outputId": "dd9486da-b039-4ee3-a2e0-3e76b50a671f"
      },
      "source": [
        "max_words= 10000\n",
        "tokenizer= Tokenizer(num_words=max_words)\n",
        "print(x_train.tolist()[:5])\n",
        "print(x_test.tolist()[:5])\n",
        "tokenizer.fit_on_sequences(x_train.tolist())\n",
        "tokenizer.fit_on_sequences(x_test.tolist())\n",
        "\n",
        "print(tokenizer.sequences_to_matrix(x_train.tolist()))\n",
        "\n",
        "x_train = np.asarray(tokenizer.sequences_to_matrix(x_train.tolist(), mode=\"tfidf\"))\n",
        "x_test = np.asarray(tokenizer.sequences_to_matrix(x_test.tolist(), mode=\"tfidf\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12], [1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 30625, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12], [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 11733, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 13051, 71, 5, 160, 63, 11, 9, 26503, 81, 5, 102, 59, 11, 17, 12], [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 465, 893, 3541, 114, 2902, 69, 312, 35, 15, 7, 335, 1679, 21, 25, 3675, 19519, 3498, 58, 69, 68, 493, 5, 25, 465, 377, 2430, 4, 293, 1172, 739, 4379, 8, 7, 1510, 1131, 13, 899, 6, 4, 990, 309, 415, 4519, 6920, 645, 3916, 791, 5, 4379, 75, 8, 24, 10, 1311, 4677, 5, 344, 756, 7, 29700, 231, 9691, 2603, 1413, 43, 509, 43, 68, 327, 5, 14560, 3498, 297, 638, 73, 430, 22, 4, 580, 7, 48, 41, 30, 14021, 136, 4, 344, 298, 4, 580, 40, 344, 5078, 23457, 291, 1488, 10, 3148, 5, 231, 6250, 1308, 5, 8250, 7043, 21, 18776, 1622, 990, 309, 415, 265, 5992, 8945, 1149, 9118, 27677, 4, 344, 9691, 756, 3729, 14560, 4667, 28400, 3249, 28, 10, 2190, 24, 77, 41, 682, 10, 4851, 2048, 7, 4, 5540, 2926, 1598, 22, 370, 5954, 7541, 5, 54, 5232, 1685, 2916, 10, 1571, 946, 60, 51, 3249, 5249, 4, 73, 2135, 669, 4, 580, 64, 10, 4280, 6, 16319, 25, 482, 35, 150, 377, 2430, 7, 10, 21743, 836, 29981, 4730, 6920, 5, 4379, 12711, 16799, 3541, 8, 4, 344, 291, 29693, 298, 4228, 6, 2223, 24, 14560, 41, 343, 430, 210, 6, 3498, 297, 64, 10, 2281, 455, 5, 7003, 125, 222, 17, 12], [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 22, 265, 9, 621, 575, 1080, 4742, 1149, 15874, 6, 438, 8295, 13, 102, 388, 15, 90, 67, 7, 197, 8295, 8, 4, 270, 416, 23, 527, 6, 15874, 4891, 4, 1055, 742, 16, 8, 36, 1480, 6, 2124, 100, 543, 5, 645, 362, 6, 2912, 4, 49, 8, 15874, 976, 124, 20, 5, 8295, 80, 9, 100, 362, 543, 395, 61, 44, 20, 8295, 8, 16, 40, 1276, 42, 1436, 166, 415, 6, 888, 4, 116, 9, 40, 3089, 4, 303, 163, 16, 64, 772, 13, 94, 156, 17, 12]]\n",
            "[[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 18292, 159, 9, 1084, 363, 13, 19231, 71, 9, 16273, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 16273, 7, 748, 48, 9, 19231, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 23406, 28185, 5, 192, 510, 17, 12], [1, 2768, 283, 122, 7, 4, 89, 544, 463, 29, 798, 748, 40, 85, 306, 28, 19, 59, 11, 82, 84, 22, 10, 1315, 19, 12, 11, 82, 52, 29, 283, 1135, 558, 13086, 265, 27151, 6607, 8, 6607, 118, 371, 10, 1503, 281, 4, 143, 4811, 760, 50, 2088, 225, 139, 683, 4, 48, 193, 862, 41, 967, 1999, 30, 1086, 36, 8, 28, 602, 19, 32, 11, 82, 5, 4, 89, 544, 463, 41, 30, 6273, 13, 260, 951, 6607, 8, 69, 1749, 18, 82, 41, 30, 306, 3342, 13, 4, 37, 38, 283, 555, 649, 18, 82, 13, 1721, 282, 9, 132, 18, 82, 41, 30, 385, 21, 4, 169, 76, 36, 8, 107, 4, 106, 524, 10, 295, 3825, 13086, 2476, 6, 3684, 6940, 4, 1126, 41, 263, 84, 395, 649, 18, 82, 838, 1317, 4, 572, 4, 106, 13, 25, 595, 2445, 40, 85, 7369, 518, 5, 4, 1126, 51, 115, 680, 16, 6, 719, 250, 27, 429, 6607, 8, 6940, 114, 343, 84, 142, 20, 5, 1145, 1538, 4, 65, 494, 474, 27, 69, 445, 11, 1816, 6607, 8, 109, 181, 2768, 15921, 62, 1810, 6, 624, 901, 6940, 107, 4, 1126, 34, 524, 4, 6940, 1126, 41, 447, 7, 1427, 13, 69, 251, 18, 872, 876, 1539, 468, 9063, 242, 5, 646, 27, 1888, 169, 283, 87, 9, 10, 29487, 260, 182, 122, 678, 306, 13, 4, 99, 216, 7, 89, 544, 64, 85, 2333, 6, 195, 7254, 6337, 268, 609, 4, 195, 41, 1017, 2765, 17922, 4, 73, 706, 16980, 92, 4, 91, 3917, 36, 8, 51, 144, 23, 1858, 129, 564, 13, 269, 678, 115, 55, 866, 189, 814, 604, 838, 117, 380, 595, 951, 320, 4, 398, 57, 2233, 7411, 269, 274, 87, 6607, 8, 787, 283, 34, 596, 661, 5467, 13, 2362, 1816, 90, 20819, 84, 22, 2202, 1816, 54, 748, 6607, 8, 87, 62, 6154, 84, 161, 5, 1208, 480, 4, 30624, 416, 6, 538, 122, 115, 55, 129, 1104, 1445, 345, 389, 31, 4, 169, 76, 36, 8, 787, 398, 7, 4, 24299, 1507, 64, 8862, 22, 125, 19042, 9, 2876, 172, 399, 9, 29058, 5206, 9, 19541, 122, 36, 8, 6642, 172, 247, 100, 97, 6940, 34, 75, 477, 541, 4, 283, 182, 4, 30840, 295, 301, 30755, 125, 18545, 6607, 8, 77, 57, 445, 283, 1998, 217, 31, 380, 704, 51, 77, 12689, 509, 5, 476, 9, 2876, 122, 115, 853, 6, 1061, 52, 10, 25308, 29198, 1308, 5, 4, 283, 182, 36, 8, 5296, 114, 30, 531, 6, 6376, 9, 2470, 529, 13, 21924, 22901, 58, 529, 7, 2148, 22335, 185, 1028, 240, 5296, 1028, 949, 657, 57, 6, 1046, 283, 36, 8, 6607, 8, 4, 2217, 34, 9177, 13, 10, 4910, 5, 4, 141, 283, 120, 50, 2877, 7, 1049, 43, 10, 181, 283, 734, 115, 55, 3356, 476, 6, 2195, 10, 73, 120, 50, 41, 6877, 169, 87, 6607, 8, 107, 144, 23, 129, 120, 169, 87, 33, 2409, 30, 1888, 1171, 161, 4, 294, 517, 23, 12283, 25, 398, 9, 2060, 283, 21, 4, 236, 36, 8, 143, 169, 87, 641, 1569, 28, 69, 61, 376, 514, 90, 1249, 62, 12044, 13, 4, 2217, 696, 122, 404, 2936, 22, 134, 6, 187, 514, 10, 1249, 107, 4, 96, 1043, 1569, 13, 10, 184, 28, 61, 376, 514, 268, 680, 4, 320, 6, 154, 6, 69, 160, 514, 10, 1249, 27, 4, 153, 5, 52, 29, 36, 8, 6607, 8, 612, 408, 10, 3133, 283, 76, 27, 1504, 31, 169, 951, 15614, 122, 36, 8, 283, 236, 62, 641, 84, 618, 19833, 22, 8417, 8409, 9, 274, 7322, 399, 7587, 51, 115, 55, 45, 4044, 31, 4, 490, 558, 36, 8, 224, 29318, 115, 57, 85, 1655, 2671, 5, 283, 6, 4, 37, 38, 7, 1797, 185, 77, 4446, 4, 555, 298, 77, 240, 10892, 7, 327, 652, 194, 8773, 6233, 34, 14152, 5463, 4884, 1297, 6, 240, 260, 458, 87, 6, 134, 514, 10, 1249, 22, 196, 514, 4, 37, 38, 309, 213, 54, 207, 8577, 25, 134, 139, 89, 283, 494, 555, 22, 4, 2217, 6, 2172, 4278, 434, 835, 22, 3598, 3746, 434, 835, 7, 48, 6607, 8, 618, 225, 586, 333, 122, 572, 126, 2768, 1998, 62, 133, 6, 2458, 233, 28, 602, 188, 5, 4, 704, 1998, 62, 45, 885, 281, 4, 48, 193, 760, 36, 8, 115, 680, 78, 58, 109, 95, 6, 1732, 1516, 281, 4, 225, 760, 17, 12], [1, 4, 309, 2276, 4759, 5, 2015, 403, 1920, 33, 1575, 1627, 1173, 87, 13, 536, 78, 6490, 399, 7, 2068, 212, 10, 634, 179, 8, 137, 5602, 7, 2775, 33, 30, 1015, 43, 33, 5602, 50, 489, 4, 403, 6, 96, 399, 7, 1953, 3587, 8427, 6603, 4132, 3669, 8180, 7163, 9, 2015, 8, 12424, 12296, 1683, 791, 5, 740, 220, 707, 13, 4, 634, 634, 54, 1405, 6331, 4, 361, 182, 24, 511, 972, 137, 403, 1920, 529, 6, 96, 3711, 399, 41, 30, 2776, 21, 10, 8491, 2002, 503, 5, 188, 6, 353, 26, 2474, 21, 432, 4, 4234, 23, 3288, 435, 34, 737, 6, 246, 7528, 274, 1173, 1627, 87, 13, 399, 992, 27, 274, 403, 87, 2631, 85, 480, 52, 2015, 403, 820, 13, 10, 139, 9, 115, 949, 609, 890, 819, 6, 812, 593, 7, 576, 7, 194, 2329, 216, 12296, 8, 12296, 8, 634, 33, 768, 2085, 593, 4, 403, 1920, 185, 9, 107, 403, 87, 12089, 107, 1635, 410, 4, 682, 189, 161, 1635, 762, 274, 5319, 115, 30, 43, 389, 410, 4, 682, 107, 1635, 762, 456, 36, 8, 184, 4057, 95, 1854, 107, 403, 87, 302, 12296, 8, 129, 100, 756, 7, 3288, 96, 298, 55, 370, 731, 866, 189, 115, 949, 9695, 115, 949, 343, 756, 14738, 9, 115, 949, 343, 756, 2509, 36, 8, 17, 12], [1, 11786, 13716, 65, 9, 249, 1096, 8, 16, 515, 4, 211, 5, 881, 7, 78, 181, 65, 9, 249, 1441, 6, 56, 387, 280, 141, 81, 13, 360, 11, 15, 4, 49, 8, 16, 385, 69, 68, 327, 5, 25, 132, 20, 128, 7, 4, 6452, 3254, 9, 681, 30587, 1441, 25, 78, 473, 814, 1110, 16, 8, 16, 529, 69, 119, 11, 15, 5, 4, 1154, 6, 16657, 1679, 21, 25, 645, 4005, 975, 2170, 303, 1679, 464, 21, 4, 1004, 279, 11, 15, 5, 975, 57, 85, 2589, 457, 207, 448, 43, 10, 447, 16, 8, 4, 49, 8, 4, 123, 755, 62, 4198, 6, 960, 42, 2559, 5, 126, 231, 8326, 9, 42, 148, 5, 128, 6, 165, 44, 20, 22, 165, 70, 20, 457, 207, 1504, 16, 8, 4, 15576, 2049, 96, 13, 19, 1340, 18, 1379, 34, 75, 717, 6, 132, 26, 22, 19, 132, 15, 4, 49, 8, 387, 280, 200, 6, 67, 4, 404, 5, 879, 122, 28, 4, 6452, 3254, 979, 17, 12], [1, 470, 354, 18270, 4231, 62, 2373, 509, 1687, 5138, 7, 4, 4893, 410, 4, 8178, 9, 2185, 9371, 7, 68, 5, 4, 2414, 9676, 1025, 7, 4, 4051, 13, 509, 206, 4, 8178, 166, 5, 4821, 8, 7, 5271, 9, 2774, 84, 6, 132, 1687, 62, 257, 6, 30, 8908, 7, 4, 4893, 9, 657, 4, 473, 5, 4, 10646, 17767, 62, 1465, 2125, 7, 4897, 250, 6, 4, 10699, 1094, 13380, 173, 8, 290, 9676, 662, 7, 4, 1033, 4051, 28, 4, 460, 65, 1843, 5, 13159, 9, 14185, 64, 2243, 77, 8, 859, 173, 7, 7212, 2774, 8, 4, 9676, 662, 7, 4, 4051, 62, 4, 2414, 13, 124, 206, 31, 1094, 4898, 10, 3274, 2598, 6, 933, 1564, 7, 4, 2308, 9334, 5, 4, 403, 5, 18112, 50, 19689, 5271, 9, 2774, 4, 4893, 23, 84, 6, 68, 13764, 9764, 31, 12348, 9, 20600, 8030, 16, 172, 967, 17724, 9652, 109, 4165, 274, 2185, 13380, 173, 8, 859, 1190, 452, 4231, 91, 1671, 281, 4, 1438, 51, 10, 226, 499, 7, 9064, 114, 1099, 871, 6, 10, 4748, 4, 173, 8, 17, 12]]\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA0skK5bX5Wz",
        "outputId": "7f2c1b57-7506-45d7-f61f-5491f646437c"
      },
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (8982, 10000)\n",
            "x_test shape: (2246, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prmi-Adnd4gP",
        "outputId": "5e460eaf-b021-4995-c596-d33868d581b8"
      },
      "source": [
        "print(x_train[:5])\n",
        "print(x_test[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]]\n",
            "[[0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.69310265 0.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9msddbWd9af"
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
        "y_test = to_categorical(y_test, num_classes) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfHwQgfOgZUJ",
        "outputId": "dfce430f-ed07-42c5-b27e-84cb5b15424d"
      },
      "source": [
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PilLUXeJga7j"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtclUWCagdez"
      },
      "source": [
        "# 모델은 인터넷에서 찾아본것 중 테스트 정확도가 높게 나온걸 사용했다..\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxjgkT1CgpTZ"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 훈련 조기 종료\n",
        "mc = ModelCheckpoint('t_best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 훈련 과정에서 검증 데이터가 가장 높았을 때 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7tacQx_gsIS"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c78dvqfhguM7",
        "outputId": "1604216d-cb80-45b2-b62b-4a8fae6c6562"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - 6s 73ms/step - loss: 1.8900 - acc: 0.6129 - val_loss: 0.9047 - val_acc: 0.8121\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.81211, saving model to t_best_model.h5\n",
            "Epoch 2/30\n",
            "71/71 [==============================] - 5s 69ms/step - loss: 0.3896 - acc: 0.9200 - val_loss: 0.9296 - val_acc: 0.8143\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.81211 to 0.81434, saving model to t_best_model.h5\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - 5s 68ms/step - loss: 0.2335 - acc: 0.9484 - val_loss: 0.9907 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.81434\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - 5s 67ms/step - loss: 0.1981 - acc: 0.9539 - val_loss: 1.0522 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.81434\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - 5s 68ms/step - loss: 0.1823 - acc: 0.9585 - val_loss: 1.0929 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.81434\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3KGJOAmgwCE",
        "outputId": "9c84402b-313e-439a-b2e2-423869580e21"
      },
      "source": [
        "loaded_model = load_model('t_best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 1s 19ms/step - loss: 0.9296 - acc: 0.8143\n",
            "\n",
            " 테스트 정확도: 0.8143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGxUxauHhQnq"
      },
      "source": [
        "#결과 : 0.8143"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQT0-6CCh3YR"
      },
      "source": [
        "# TF-IDF-ADAMAX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_a8Ike4hxmy"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6dyRdcghyV5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47kvoHWJh1EQ",
        "outputId": "1a9735d7-035d-44c1-c60c-e956a1df6ec9"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:144: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEvdcxFvh-HY"
      },
      "source": [
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKyvmmTQiALE",
        "outputId": "94cf6d74-1ce2-4bf0-a30e-f3db5c0c7249"
      },
      "source": [
        "print(f'훈련 샘플의 수 : {len(x_train)}') # train sequences\n",
        "print(f'테스트 샘플의 수 : {len(x_test)}') # test sequences\n",
        "num_classes = max(y_train) + 1\n",
        "print(f'클래스의 수 : {num_classes}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플의 수 : 8982\n",
            "테스트 샘플의 수 : 2246\n",
            "클래스의 수 : 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Wp-aXdiCVe"
      },
      "source": [
        "index_to_word = {}\n",
        "for key, value in word_index.items():\n",
        "    index_to_word[value] = key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFAKsGIQiD4x",
        "outputId": "6e74413b-028a-45dd-d853-ef9fc5016d6a"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8982,)\n",
            "(2246,)\n",
            "(8982,)\n",
            "(2246,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4YvaCQ0iFha",
        "outputId": "4771226c-e55b-4501-8f0b-e55e38f7d826"
      },
      "source": [
        "print(' '.join([index_to_word[x] for x in x_train[0]]))\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_RpKWQ1iHzN"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRDjG9S0iJs6"
      },
      "source": [
        "# Vectorizing sequence data\n",
        "max_words= 10000\n",
        "tokenizer= Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_sequences(x_train.tolist())\n",
        "tokenizer.fit_on_sequences(x_test.tolist())\n",
        "x_train = np.asarray(tokenizer.sequences_to_matrix(x_train.tolist(), mode=\"tfidf\"))\n",
        "x_test = np.asarray(tokenizer.sequences_to_matrix(x_test.tolist(), mode=\"tfidf\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2s01PfuiN3k",
        "outputId": "0324043a-3008-410e-ad89-98166f454f82"
      },
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (8982, 10000)\n",
            "x_test shape: (2246, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsTcjzGbiQ3O"
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes) # 훈련용 뉴스 기사 레이블의 원-핫 인코딩\n",
        "y_test = to_categorical(y_test, num_classes) # 테스트용 뉴스 기사 레이블의 원-핫 인코딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsVcgs0WiShr",
        "outputId": "d75eb635-9aef-48eb-9dcc-80d6de85ffc3"
      },
      "source": [
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnJld2cmiWa2"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gADJqdIiYKg"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwhSyYLAid27"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 훈련 조기 종료\n",
        "mc = ModelCheckpoint('./t_best_model_adamax.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 훈련 과정에서 검증 데이터가 가장 높았을 때 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkZdLr1Sif9A"
      },
      "source": [
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['acc']) # Adamax # 무한 노름(infinity norm)에 기반한 Adam의 변형"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEnTXXeOim-0",
        "outputId": "12ed0383-28fa-487a-eca2-e701f2508596"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=128, epochs=100, callbacks=[es, mc], validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "71/71 [==============================] - 6s 73ms/step - loss: 2.1092 - acc: 0.5723 - val_loss: 1.1281 - val_acc: 0.7614\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.76135, saving model to ./t_best_model_adamax.h5\n",
            "Epoch 2/100\n",
            "71/71 [==============================] - 5s 68ms/step - loss: 0.7998 - acc: 0.8342 - val_loss: 0.9495 - val_acc: 0.7965\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.76135 to 0.79653, saving model to ./t_best_model_adamax.h5\n",
            "Epoch 3/100\n",
            "71/71 [==============================] - 5s 68ms/step - loss: 0.5261 - acc: 0.8934 - val_loss: 0.8836 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.79653 to 0.80810, saving model to ./t_best_model_adamax.h5\n",
            "Epoch 4/100\n",
            "71/71 [==============================] - 5s 69ms/step - loss: 0.3931 - acc: 0.9196 - val_loss: 0.8647 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.80810 to 0.81077, saving model to ./t_best_model_adamax.h5\n",
            "Epoch 5/100\n",
            "71/71 [==============================] - 5s 68ms/step - loss: 0.2952 - acc: 0.9406 - val_loss: 0.8644 - val_acc: 0.8117\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.81077 to 0.81167, saving model to ./t_best_model_adamax.h5\n",
            "Epoch 6/100\n",
            "71/71 [==============================] - 5s 69ms/step - loss: 0.2455 - acc: 0.9453 - val_loss: 0.8683 - val_acc: 0.8139\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.81167 to 0.81389, saving model to ./t_best_model_adamax.h5\n",
            "Epoch 7/100\n",
            "71/71 [==============================] - 5s 67ms/step - loss: 0.2063 - acc: 0.9540 - val_loss: 0.8733 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.81389\n",
            "Epoch 8/100\n",
            "71/71 [==============================] - 5s 68ms/step - loss: 0.1880 - acc: 0.9530 - val_loss: 0.8957 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.81389\n",
            "Epoch 9/100\n",
            "71/71 [==============================] - 5s 68ms/step - loss: 0.1742 - acc: 0.9530 - val_loss: 0.9005 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.81389 to 0.81523, saving model to ./t_best_model_adamax.h5\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWuby9y5ipCj",
        "outputId": "120c3112-a2fb-4ddb-d39e-482be5c3ee61"
      },
      "source": [
        "loaded_model = load_model('t_best_model_adamax.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 1s 10ms/step - loss: 0.9005 - acc: 0.8152\n",
            "\n",
            " 테스트 정확도: 0.8152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ufBcmDkDHb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSwxmZh0jfOx"
      },
      "source": [
        "# 결과: 0.8152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1jqaM3j-JE"
      },
      "source": [
        "#TF-IDF- SGD 확률적 경사하강법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "macHQmpZlf8X"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 훈련 조기 종료\n",
        "mc = ModelCheckpoint('./t_best_model_sgd.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 훈련 과정에서 검증 데이터가 가장 높았을 때 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12ZgCkiSjzSy"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])  # 확률적 경사 하강법 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgpyOSl-kH3k",
        "outputId": "185d0113-2cff-4700-eeb7-bbdd46677d0b"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=128, epochs=100, callbacks=[es, mc], validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "71/71 [==============================] - 7s 92ms/step - loss: 0.1224 - acc: 0.9668 - val_loss: 1.6833 - val_acc: 0.8028\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.80276, saving model to ./t_best_model_sgd.h5\n",
            "Epoch 2/100\n",
            "71/71 [==============================] - 5s 71ms/step - loss: 0.1433 - acc: 0.9638 - val_loss: 1.6633 - val_acc: 0.8032\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.80276 to 0.80321, saving model to ./t_best_model_sgd.h5\n",
            "Epoch 3/100\n",
            "71/71 [==============================] - 7s 92ms/step - loss: 0.1596 - acc: 0.9612 - val_loss: 1.6465 - val_acc: 0.8032\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.80321\n",
            "Epoch 4/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1354 - acc: 0.9626 - val_loss: 1.6348 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.80321 to 0.80410, saving model to ./t_best_model_sgd.h5\n",
            "Epoch 5/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1267 - acc: 0.9639 - val_loss: 1.6297 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.80410 to 0.80499, saving model to ./t_best_model_sgd.h5\n",
            "Epoch 6/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1226 - acc: 0.9668 - val_loss: 1.6176 - val_acc: 0.8045\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.80499\n",
            "Epoch 7/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.1207 - acc: 0.9641 - val_loss: 1.6061 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.80499 to 0.80543, saving model to ./t_best_model_sgd.h5\n",
            "Epoch 8/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.1112 - acc: 0.9671 - val_loss: 1.6052 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.80543 to 0.80632, saving model to ./t_best_model_sgd.h5\n",
            "Epoch 9/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1066 - acc: 0.9676 - val_loss: 1.6064 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.80632\n",
            "Epoch 10/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.0959 - acc: 0.9694 - val_loss: 1.6016 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.80632\n",
            "Epoch 11/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1011 - acc: 0.9674 - val_loss: 1.6020 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.80632\n",
            "Epoch 12/100\n",
            "71/71 [==============================] - 4s 59ms/step - loss: 0.1012 - acc: 0.9666 - val_loss: 1.5984 - val_acc: 0.8068\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.80632 to 0.80677, saving model to ./t_best_model_sgd.h5\n",
            "Epoch 13/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.1118 - acc: 0.9622 - val_loss: 1.5930 - val_acc: 0.8068\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.80677\n",
            "Epoch 14/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.0930 - acc: 0.9657 - val_loss: 1.5942 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.80677\n",
            "Epoch 15/100\n",
            "71/71 [==============================] - 4s 59ms/step - loss: 0.1043 - acc: 0.9633 - val_loss: 1.5939 - val_acc: 0.8068\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.80677\n",
            "Epoch 16/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.0927 - acc: 0.9671 - val_loss: 1.5918 - val_acc: 0.8059\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.80677\n",
            "Epoch 17/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1038 - acc: 0.9630 - val_loss: 1.5957 - val_acc: 0.8068\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.80677\n",
            "Epoch 18/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.1000 - acc: 0.9626 - val_loss: 1.5942 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.80677\n",
            "Epoch 19/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1059 - acc: 0.9603 - val_loss: 1.5924 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.80677\n",
            "Epoch 20/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.0985 - acc: 0.9663 - val_loss: 1.5887 - val_acc: 0.8068\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.80677\n",
            "Epoch 21/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.1004 - acc: 0.9633 - val_loss: 1.5811 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.80677\n",
            "Epoch 22/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.1020 - acc: 0.9635 - val_loss: 1.5806 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.80677\n",
            "Epoch 23/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.0880 - acc: 0.9675 - val_loss: 1.5826 - val_acc: 0.8050\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.80677\n",
            "Epoch 24/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.0890 - acc: 0.9677 - val_loss: 1.5822 - val_acc: 0.8068\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.80677\n",
            "Epoch 25/100\n",
            "71/71 [==============================] - 4s 61ms/step - loss: 0.0913 - acc: 0.9625 - val_loss: 1.5827 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.80677\n",
            "Epoch 26/100\n",
            "71/71 [==============================] - 4s 60ms/step - loss: 0.0909 - acc: 0.9647 - val_loss: 1.5880 - val_acc: 0.8041\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.80677\n",
            "Epoch 00026: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM51tvnHlOA6",
        "outputId": "bc070b05-b9fd-4ded-aecb-68fb80d3b270"
      },
      "source": [
        "loaded_model = load_model('t_best_model_sgd.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 1s 10ms/step - loss: 1.5984 - acc: 0.8068\n",
            "\n",
            " 테스트 정확도: 0.8068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMKyxawFmdh0"
      },
      "source": [
        "# 결과 : 0.8068"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r9HCPoGk5SB"
      },
      "source": [
        "#TF-IDF RMS prop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KddRQMBhmuAM"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4) # 훈련 조기 종료\n",
        "mc = ModelCheckpoint('./t_best_model_RMS.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) # 훈련 과정에서 검증 데이터가 가장 높았을 때 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk2KTDkykMxN"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc']) # RMSProp # 일반적으로 순환 신경망(Recurrent Neural Networks)의 옵티마이저로 많이 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXxO0BCekvaQ",
        "outputId": "66c9611e-706d-44da-fa1a-6e28e8a1e40b"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=128, epochs=100, callbacks=[es, mc], validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "71/71 [==============================] - 7s 93ms/step - loss: 0.1734 - acc: 0.9575 - val_loss: 1.7994 - val_acc: 0.8054\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.80543, saving model to ./t_best_model_RMS.h5\n",
            "Epoch 2/100\n",
            "71/71 [==============================] - 6s 88ms/step - loss: 0.1791 - acc: 0.9558 - val_loss: 1.9421 - val_acc: 0.8023\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.80543\n",
            "Epoch 3/100\n",
            "71/71 [==============================] - 6s 89ms/step - loss: 0.1725 - acc: 0.9583 - val_loss: 2.0778 - val_acc: 0.7983\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.80543\n",
            "Epoch 4/100\n",
            "71/71 [==============================] - 6s 88ms/step - loss: 0.1632 - acc: 0.9619 - val_loss: 2.2791 - val_acc: 0.7965\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.80543\n",
            "Epoch 5/100\n",
            "71/71 [==============================] - 6s 89ms/step - loss: 0.1691 - acc: 0.9569 - val_loss: 2.4562 - val_acc: 0.7934\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.80543\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yu41Uj8lDha",
        "outputId": "9fe50550-ec11-4db0-8af2-2ab5784f330b"
      },
      "source": [
        "loaded_model = load_model('t_best_model_RMS.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 1s 10ms/step - loss: 1.7994 - acc: 0.8054\n",
            "\n",
            " 테스트 정확도: 0.8054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtAHwDurnGre"
      },
      "source": [
        "# 결과 : 0.8054"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQcWbG9wj0-1"
      },
      "source": [
        "# Word2Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2lTY7fAqzHV"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEmd4Qq4rIVK",
        "outputId": "c8a400ba-f171-4e3e-d78b-71812642c963"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:144: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USzhIYLOqzEr"
      },
      "source": [
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGOSAiFVq6r5",
        "outputId": "be0862cf-e16f-4e6a-deba-f76b6c0e9d42"
      },
      "source": [
        "print(f'훈련 샘플의 수 : {len(x_train)}') # train sequences\n",
        "print(f'테스트 샘플의 수 : {len(x_test)}') # test sequences\n",
        "num_classes = max(y_train) + 1\n",
        "print(f'클래스의 수 : {num_classes}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플의 수 : 8982\n",
            "테스트 샘플의 수 : 2246\n",
            "클래스의 수 : 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez9SJV5uq9mw"
      },
      "source": [
        "index_to_word = {}\n",
        "for key, value in word_index.items():\n",
        "    index_to_word[value] = key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJrHUlKFrUFJ",
        "outputId": "6395348f-4595-4528-89ec-dc3ab96da30c"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(x_train[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8982,)\n",
            "(2246,)\n",
            "(8982,)\n",
            "(2246,)\n",
            "[list([1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])\n",
            " list([1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 30625, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12])\n",
            " list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 11733, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 13051, 71, 5, 160, 63, 11, 9, 26503, 81, 5, 102, 59, 11, 17, 12])\n",
            " list([1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 465, 893, 3541, 114, 2902, 69, 312, 35, 15, 7, 335, 1679, 21, 25, 3675, 19519, 3498, 58, 69, 68, 493, 5, 25, 465, 377, 2430, 4, 293, 1172, 739, 4379, 8, 7, 1510, 1131, 13, 899, 6, 4, 990, 309, 415, 4519, 6920, 645, 3916, 791, 5, 4379, 75, 8, 24, 10, 1311, 4677, 5, 344, 756, 7, 29700, 231, 9691, 2603, 1413, 43, 509, 43, 68, 327, 5, 14560, 3498, 297, 638, 73, 430, 22, 4, 580, 7, 48, 41, 30, 14021, 136, 4, 344, 298, 4, 580, 40, 344, 5078, 23457, 291, 1488, 10, 3148, 5, 231, 6250, 1308, 5, 8250, 7043, 21, 18776, 1622, 990, 309, 415, 265, 5992, 8945, 1149, 9118, 27677, 4, 344, 9691, 756, 3729, 14560, 4667, 28400, 3249, 28, 10, 2190, 24, 77, 41, 682, 10, 4851, 2048, 7, 4, 5540, 2926, 1598, 22, 370, 5954, 7541, 5, 54, 5232, 1685, 2916, 10, 1571, 946, 60, 51, 3249, 5249, 4, 73, 2135, 669, 4, 580, 64, 10, 4280, 6, 16319, 25, 482, 35, 150, 377, 2430, 7, 10, 21743, 836, 29981, 4730, 6920, 5, 4379, 12711, 16799, 3541, 8, 4, 344, 291, 29693, 298, 4228, 6, 2223, 24, 14560, 41, 343, 430, 210, 6, 3498, 297, 64, 10, 2281, 455, 5, 7003, 125, 222, 17, 12])\n",
            " list([1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 22, 265, 9, 621, 575, 1080, 4742, 1149, 15874, 6, 438, 8295, 13, 102, 388, 15, 90, 67, 7, 197, 8295, 8, 4, 270, 416, 23, 527, 6, 15874, 4891, 4, 1055, 742, 16, 8, 36, 1480, 6, 2124, 100, 543, 5, 645, 362, 6, 2912, 4, 49, 8, 15874, 976, 124, 20, 5, 8295, 80, 9, 100, 362, 543, 395, 61, 44, 20, 8295, 8, 16, 40, 1276, 42, 1436, 166, 415, 6, 888, 4, 116, 9, 40, 3089, 4, 303, 163, 16, 64, 772, 13, 94, 156, 17, 12])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmOVTPfHrWau"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbMi0TXPfn1R",
        "outputId": "9fd3c02d-3818-4f2d-c3b5-6d768c55c4ce"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
        "\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "[list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])\n",
            " list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12])\n",
            " list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12])\n",
            " ...\n",
            " list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12])\n",
            " list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12])\n",
            " list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])]\n",
            "[ 3  4  3 ... 25  3 25]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:143: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/reuters.py:144: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNaXus7jhjHO",
        "outputId": "170346dd-a9cf-4d90-c100-8a955a31200c"
      },
      "source": [
        "# 분류된 기사 종류의 수가 46개\n",
        "num_classes = max(y_train) + 1\n",
        "print('클래스의 수 : {}'.format(num_classes))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "클래스의 수 : 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdQYps4Rhn-o",
        "outputId": "4b2c43b0-c559-4309-94ef-82021c4ca0ee"
      },
      "source": [
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtGsgLa5hpgO"
      },
      "source": [
        "index_to_word = {index +3 : word for word, index in word_index.items()}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcHSdtp0hscO",
        "outputId": "9354ecbe-2bae-44b0-e9ec-d60fe9f9e6ef"
      },
      "source": [
        "# 로이터의 데이타 특성을 위해 3가지 종류를 index_to_word에 추가\n",
        "# 0 <pad>\n",
        "# 1 <sos>\n",
        "# 2 <unk>\n",
        "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "  index_to_word[index] = token\n",
        "\n",
        "print(x_train[0])\n",
        "print(index_to_word[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kts3O4x6hyKu",
        "outputId": "5ebd65cc-ae13-47ae-808e-42a0cfaf2b00"
      },
      "source": [
        "print(' '.join([index_to_word[index] for index in x_train[0]]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> <unk> <unk> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Cu3Z9Yh1GW"
      },
      "source": [
        "decoded = []\n",
        "for i in range(len(x_train)):\n",
        "  t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
        "  decoded.append(t)\n",
        "\n",
        "x_train = decoded"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmnzik3jh23f"
      },
      "source": [
        "decoded = []\n",
        "for i in range(len(x_test)):\n",
        "  t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
        "  decoded.append(t)\n",
        "\n",
        "x_test = decoded"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SkBBE16h9rO",
        "outputId": "61c9c06e-5a5a-435b-8fd4-84a3cdb5796f"
      },
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('abc')\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_data = []\n",
        "for sentence in x_train:\n",
        "    temp_X = word_tokenize(sentence) # 토큰화\n",
        "    temp_X = [word for word in temp_X ]   \n",
        "    tokenized_data.append(temp_X)\n",
        "\n",
        "tokenized_data2 = []\n",
        "for sentence in x_test:\n",
        "    temp_X = word_tokenize(sentence) # 토큰화\n",
        "    temp_X = [word for word in temp_X ]   \n",
        "    tokenized_data2.append(temp_X)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Package abc is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq0uNj7wrcyI",
        "outputId": "bcc82144-4aff-4cf7-cccd-e23420e62e4c"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model1 = Word2Vec(sentences = tokenized_data, window = 5, min_count = 5, workers = 4, sg = 0)\n",
        "model2 = Word2Vec(sentences = tokenized_data2, window = 5, min_count = 5, workers = 4, sg = 0)\n",
        "\n",
        "print(model1.vector_size)\n",
        "print(model2.vector_size)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sRqSsXdr9mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a740dd-e44c-4904-edb7-f945cb4b58b4"
      },
      "source": [
        "print(len(tokenized_data))\n",
        "print(tokenized_data[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8982\n",
            "[['<', 'sos', '>', '<', 'unk', '>', '<', 'unk', '>', 'said', 'as', 'a', 'result', 'of', 'its', 'december', 'acquisition', 'of', 'space', 'co', 'it', 'expects', 'earnings', 'per', 'share', 'in', '1987', 'of', '1', '15', 'to', '1', '30', 'dlrs', 'per', 'share', 'up', 'from', '70', 'cts', 'in', '1986', 'the', 'company', 'said', 'pretax', 'net', 'should', 'rise', 'to', 'nine', 'to', '10', 'mln', 'dlrs', 'from', 'six', 'mln', 'dlrs', 'in', '1986', 'and', 'rental', 'operation', 'revenues', 'to', '19', 'to', '22', 'mln', 'dlrs', 'from', '12', '5', 'mln', 'dlrs', 'it', 'said', 'cash', 'flow', 'per', 'share', 'this', 'year', 'should', 'be', '2', '50', 'to', 'three', 'dlrs', 'reuter', '3'], ['<', 'sos', '>', 'generale', 'de', 'banque', 'sa', 'lt', '<', 'unk', '>', 'br', 'and', 'lt', 'heller', 'overseas', 'corp', 'of', 'chicago', 'have', 'each', 'taken', '50', 'pct', 'stakes', 'in', '<', 'unk', '>', 'company', 'sa', '<', 'unk', '>', 'factors', 'generale', 'de', 'banque', 'said', 'in', 'a', 'statement', 'it', 'gave', 'no', 'financial', 'details', 'of', 'the', 'transaction', 'sa', '<', 'unk', '>', '<', 'unk', '>', 'turnover', 'in', '1986', 'was', '17', '5', 'billion', 'belgian', 'francs', 'reuter', '3'], ['<', 'sos', '>', 'shr', '3', '28', 'dlrs', 'vs', '22', 'cts', 'shr', 'diluted', '2', '99', 'dlrs', 'vs', '22', 'cts', 'net', '46', '0', 'mln', 'vs', '3', '328', '000', 'avg', 'shrs', '14', '0', 'mln', 'vs', '15', '2', 'mln', 'year', 'shr', '5', '41', 'dlrs', 'vs', '1', '56', 'dlrs', 'shr', 'diluted', '4', '94', 'dlrs', 'vs', '1', '50', 'dlrs', 'net', '78', '2', 'mln', 'vs', '25', '9', 'mln', 'avg', 'shrs', '14', '5', 'mln', 'vs', '15', '1', 'mln', 'note', 'earnings', 'per', 'share', 'reflect', 'the', 'two', 'for', 'one', 'split', 'effective', 'january', '6', '1987', 'per', 'share', 'amounts', 'are', 'calculated', 'after', 'preferred', 'stock', 'dividends', 'loss', 'continuing', 'operations', 'for', 'the', 'qtr', '1986', 'includes', 'gains', 'of', 'sale', 'of', 'investments', 'in', '<', 'unk', '>', 'corp', 'of', '14', 'mln', 'dlrs', 'and', 'associated', 'companies', 'of', '4', '189', '000', 'less', 'writedowns', 'of', 'investments', 'in', 'national', '<', 'unk', '>', 'inc', 'of', '11', '8', 'mln', 'and', '<', 'unk', '>', 'corp', 'of', '15', '6', 'mln', 'reuter', '3'], ['<', 'sos', '>', 'the', 'farmers', 'home', 'administration', 'the', 'u', 's', 'agriculture', 'department', \"'s\", 'farm', 'lending', 'arm', 'could', 'lose', 'about', 'seven', 'billion', 'dlrs', 'in', 'outstanding', 'principal', 'on', 'its', 'severely', '<', 'unk', '>', 'borrowers', 'or', 'about', 'one', 'fourth', 'of', 'its', 'farm', 'loan', 'portfolio', 'the', 'general', 'accounting', 'office', 'gao', 'said', 'in', 'remarks', 'prepared', 'for', 'delivery', 'to', 'the', 'senate', 'agriculture', 'committee', 'brian', 'crowley', 'senior', 'associate', 'director', 'of', 'gao', 'also', 'said', 'that', 'a', 'preliminary', 'analysis', 'of', 'proposed', 'changes', 'in', '<', 'unk', '>', 'financial', 'eligibility', 'standards', 'indicated', 'as', 'many', 'as', 'one', 'half', 'of', '<', 'unk', '>', 'borrowers', 'who', 'received', 'new', 'loans', 'from', 'the', 'agency', 'in', '1986', 'would', 'be', '<', 'unk', '>', 'under', 'the', 'proposed', 'system', 'the', 'agency', 'has', 'proposed', 'evaluating', '<', 'unk', '>', 'credit', 'using', 'a', 'variety', 'of', 'financial', 'ratios', 'instead', 'of', 'relying', 'solely', 'on', '<', 'unk', '>', 'ability', 'senate', 'agriculture', 'committee', 'chairman', 'patrick', 'leahy', 'd', 'vt', '<', 'unk', '>', 'the', 'proposed', 'eligibility', 'changes', 'telling', '<', 'unk', '>', 'administrator', '<', 'unk', '>', 'clark', 'at', 'a', 'hearing', 'that', 'they', 'would', 'mark', 'a', 'dramatic', 'shift', 'in', 'the', 'agency', \"'s\", 'purpose', 'away', 'from', 'being', 'farmers', \"'\", 'lender', 'of', 'last', 'resort', 'toward', 'becoming', 'a', 'big', 'city', 'bank', 'but', 'clark', 'defended', 'the', 'new', 'regulations', 'saying', 'the', 'agency', 'had', 'a', 'responsibility', 'to', '<', 'unk', '>', 'its', '70', 'billion', 'dlr', 'loan', 'portfolio', 'in', 'a', '<', 'unk', '>', 'yet', '<', 'unk', '>', 'manner', 'crowley', 'of', 'gao', '<', 'unk', '>', '<', 'unk', '>', 'arm', 'said', 'the', 'proposed', 'credit', '<', 'unk', '>', 'system', 'attempted', 'to', 'ensure', 'that', '<', 'unk', '>', 'would', 'make', 'loans', 'only', 'to', 'borrowers', 'who', 'had', 'a', 'reasonable', 'change', 'of', 'repaying', 'their', 'debt', 'reuter', '3'], ['<', 'sos', '>', 'seton', 'co', 'said', 'its', 'board', 'has', 'received', 'a', 'proposal', 'from', 'chairman', 'and', 'chief', 'executive', 'officer', 'philip', 'd', '<', 'unk', '>', 'to', 'acquire', 'seton', 'for', '15', '75', 'dlrs', 'per', 'share', 'in', 'cash', 'seton', 'said', 'the', 'acquisition', 'bid', 'is', 'subject', 'to', '<', 'unk', '>', 'arranging', 'the', 'necessary', 'financing', 'it', 'said', 'he', 'intends', 'to', 'ask', 'other', 'members', 'of', 'senior', 'management', 'to', 'participate', 'the', 'company', 'said', '<', 'unk', '>', 'owns', '30', 'pct', 'of', 'seton', 'stock', 'and', 'other', 'management', 'members', 'another', '7', '5', 'pct', 'seton', 'said', 'it', 'has', 'formed', 'an', 'independent', 'board', 'committee', 'to', 'consider', 'the', 'offer', 'and', 'has', 'deferred', 'the', 'annual', 'meeting', 'it', 'had', 'scheduled', 'for', 'march', '31', 'reuter', '3'], ['<', 'sos', '>', 'the', 'u', 's', 'agriculture', 'department', 'estimated', 'canada', \"'s\", '1986', '87', 'wheat', 'crop', 'at', '31', '85', 'mln', 'tonnes', 'vs', '31', '85', 'mln', 'tonnes', 'last', 'month', 'it', 'estimated', '1985', '86', 'output', 'at', '24', '25', 'mln', 'tonnes', 'vs', '24', '25', 'mln', 'last', 'month', 'canadian', '1986', '87', 'coarse', 'grain', 'production', 'is', 'projected', 'at', '27', '62', 'mln', 'tonnes', 'vs', '27', '62', 'mln', 'tonnes', 'last', 'month', 'production', 'in', '1985', '86', 'is', 'estimated', 'at', '24', '95', 'mln', 'tonnes', 'vs', '24', '95', 'mln', 'last', 'month', 'canadian', 'wheat', 'exports', 'in', '1986', '87', 'are', 'forecast', 'at', '19', '00', 'mln', 'tonnes', 'vs', '18', '00', 'mln', 'tonnes', 'last', 'month', 'exports', 'in', '1985', '86', 'are', 'estimated', 'at', '17', '71', 'mln', 'tonnes', 'vs', '17', '72', 'mln', 'last', 'month', 'reuter', '3'], ['<', 'sos', '>', 'lt', 'qintex', 'america', 'ltd', 'said', 'it', 'is', 'again', 'extending', 'its', 'offer', 'of', '13', 'dlrs', 'a', 'share', 'for', '3', '3', 'mln', 'princeville', 'development', 'corp', 'shares', 'until', 'today', 'from', 'yesterday', 'at', 'midnight', 'yesterday', '7', '242', '117', 'princeville', 'shares', 'had', 'been', 'tendered', 'up', 'from', '5', '887', '165', 'shares', '24', 'hours', 'earlier', 'qintex', 'said', 'it', 'is', 'extending', 'the', 'offer', 'to', 'allow', 'princeville', 'to', 'comply', 'with', 'federal', 'law', 'restricting', 'the', 'ownership', 'of', 'u', 's', 'airlines', 'by', 'non', 'u', 's', 'citizens', 'and', 'to', 'finalize', 'the', 'terms', 'and', 'conditions', 'of', 'the', 'letter', 'of', 'credit', 'or', 'bank', 'guarantee', 'required', 'under', 'the', 'previously', 'announced', 'acquisition', 'agreement', 'reuter', '3'], ['<', 'sos', '>', 'shr', '49', 'cts', 'vs', '39', 'cts', 'net', '886', '937', 'vs', '892', '323', 'revs', '25', '9', 'mln', 'vs', '23', '7', 'mln', 'year', 'shr', '1', '78', 'dlr', 'vs', '1', '34', 'dlr', 'net', '3', '254', '301', 'vs', '2', '472', '676', 'revs', '100', '6', 'mln', 'vs', '87', '4', 'mln', 'note', '1986', '4th', 'qtr', 'and', 'year', 'net', 'includes', 'income', 'loss', 'of', '<', 'unk', '>', 'subsidiary', 'of', '14', '881', 'dlrs', 'and', '311', '848', 'dlrs', 'or', '17', 'cts', 'per', 'share', 'respectively', '1985', '4th', 'qtr', 'and', 'year', 'net', 'includes', 'loss', 'in', '<', 'unk', '>', 'unit', 'of', '108', '598', 'dlrs', 'and', '298', '412', 'dlrs', 'or', '16', 'cts', 'per', 'share', 'respectively', 'reuter', '3'], ['<', 'sos', '>', 'oper', 'shr', '23', 'cts', 'vs', '77', 'cts', 'oper', 'net', '5', '255', '179', 'vs', '17', '6', 'mln', 'revs', '37', '8', 'mln', 'vs', '73', '7', 'mln', 'note', 'cash', 'flow', '19', '5', 'mln', 'dlrs', 'or', '86', 'cts', 'shr', 'vs', '36', '7', 'mln', 'dlrs', 'or', '1', '62', 'dlrs', 'shr', '1985', 'net', 'excludes', '32', 'ct', 'shr', 'loss', 'from', 'discontinued', 'operations', 'gross', 'proven', 'and', 'probable', 'reserves', 'of', 'crude', 'oil', 'and', 'natural', 'gas', '<', 'unk', '>', '18', '4', 'mln', 'barrels', 'off', '7', '6', 'pct', 'from', 'a', 'year', 'before', 'reuter', '3'], ['<', 'sos', '>', 'lt', 'aramco', 'corp', 'has', 'accepted', 'japanese', '<', 'unk', '>', 'to', 'lift', 'a', 'higher', 'proportion', 'of', 'arab', 'heavy', 'crude', 'oil', 'under', 'term', 'contracts', 'in', 'july', 'oil', 'industry', 'sources', 'said', 'japanese', 'companies', 'requested', 'a', 'ratio', 'of', '80', 'pct', 'arab', 'heavy', 'to', '20', 'pct', 'arab', 'light', 'under', 'a', 'term', 'contract', 'agreement', 'with', 'aramco', 'for', '100', '000', 'barrels', 'per', 'day', 'the', 'sources', 'said', 'the', 'contractual', 'ratio', 'is', '30', 'pct', 'heavy', 'crude', 'to', '70', 'pct', 'light', 'japanese', 'demand', 'for', 'heavy', 'crude', 'oil', 'has', 'increased', 'substantially', 'since', 'the', 'all', 'japan', '<', 'unk', '>', 'union', 'ceased', '<', 'unk', '>', 'into', 'the', 'northern', 'mideast', 'gulf', 'last', 'month', 'causing', 'problems', 'with', 'liftings', 'of', 'heavy', 'kuwait', 'and', '<', 'unk', '>', 'crudes', 'reuter', '3']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTUngy7fjFiA"
      },
      "source": [
        "model_result = model1.wv.most_similar(\"december\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5fUHoPIjVll",
        "outputId": "fe8fc745-6992-43cb-e633-25759f76d24f"
      },
      "source": [
        "print(model_result)\n",
        "print(model.vector_size)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('january', 0.8733484148979187), ('february', 0.862957239151001), ('october', 0.8612182140350342), ('september', 0.8532811999320984), ('november', 0.8507401943206787), ('1984', 0.8181977272033691), ('august', 0.8009870052337646), ('1983', 0.7845745086669922), ('march', 0.7429894208908081), ('1980', 0.7244561910629272)]\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03mxIJedjYMo",
        "outputId": "4b7b3532-b9b6-41a6-822b-86dfd4ea9cb8"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model.wv.save_word2vec_format('./w2v')\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"./w2v\")\n",
        "print(\"모델 load완료\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "모델 load완료\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR0JTmtxjalR",
        "outputId": "012fdd60-1ace-4c3d-d1e1-6ecad1240acf"
      },
      "source": [
        "loaded_model.most_similar('acquisition')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('transaction', 0.871552586555481),\n",
              " ('merger', 0.835892915725708),\n",
              " ('sale', 0.8280469179153442),\n",
              " ('completion', 0.8188002109527588),\n",
              " ('purchase', 0.8090455532073975),\n",
              " ('offering', 0.8047310709953308),\n",
              " ('shareholder', 0.8026955127716064),\n",
              " ('buyout', 0.780207633972168),\n",
              " ('borg', 0.7714020609855652),\n",
              " ('partnership', 0.7631629705429077)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9oTQL2jji8D",
        "outputId": "2766b68c-b08b-468b-ed26-55ffa70c2845"
      },
      "source": [
        "model1.wv.vectors.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9472, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_lPyAwrjlWk",
        "outputId": "d9d90100-9413-4d77-f8f1-78a969cd061a"
      },
      "source": [
        "print(model1.wv.vectors[:5])\n",
        "print(len(model1.wv.vectors))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 8.65486383e-01  1.50054836e+00  4.33019223e-03 -3.88313055e-01\n",
            "  -1.13649988e+00 -5.22642016e-01 -1.52314448e+00  2.34618759e+00\n",
            "  -1.32605839e+00  1.08324277e+00  9.06893238e-02  7.51058102e-01\n",
            "  -2.14118052e+00 -1.15842247e+00 -5.51051497e-02 -7.29674280e-01\n",
            "   5.03455698e-01  1.03322875e+00 -1.18476498e+00  1.50206581e-01\n",
            "   1.24821830e+00  1.48341954e-01 -2.15648866e+00 -4.45207536e-01\n",
            "  -2.77271777e-01 -2.13540971e-01 -5.27371824e-01  3.65279794e-01\n",
            "  -3.74446779e-01  1.87801552e+00 -6.71217978e-01 -3.50813419e-01\n",
            "  -1.20861506e+00  1.20612524e-01  1.75407767e-01  5.40230572e-01\n",
            "  -9.03678656e-01  1.07454371e+00  1.22946650e-01 -6.60030007e-01\n",
            "  -1.20570433e+00  1.61830083e-01  1.48381278e-01  1.41275001e+00\n",
            "  -1.46235251e+00 -1.57062066e+00 -6.19161069e-01 -1.13653624e+00\n",
            "   6.08898878e-01  1.40419865e+00  1.96992442e-01 -7.12930411e-02\n",
            "  -2.56196596e-02  8.27655867e-02 -1.13556123e+00 -9.97310698e-01\n",
            "  -1.46648258e-01  1.37497509e+00 -3.62385176e-02 -4.50748056e-01\n",
            "   9.01021600e-01  3.39902610e-01 -1.07363284e+00  6.99419081e-01\n",
            "   1.04916227e+00 -2.78699756e-01 -4.12561446e-01 -7.82633841e-01\n",
            "   1.47643256e+00  1.04834780e-01 -1.79374835e-03  1.13172722e+00\n",
            "   6.65991366e-01  1.32105589e+00  8.04919660e-01 -1.74562979e+00\n",
            "   2.17903435e-01 -1.64475274e+00 -1.10943690e-01  3.57311100e-01\n",
            "  -1.52941272e-01 -1.80440271e+00  1.81531560e+00  3.13020170e-01\n",
            "   3.49051982e-01  2.86583334e-01  1.49129212e+00 -5.21940053e-01\n",
            "   3.70035395e-02 -1.21888506e+00 -8.66470575e-01  4.29307789e-01\n",
            "  -1.79621112e+00 -5.56947172e-01 -7.08536088e-01  4.51518893e-01\n",
            "  -8.16204324e-02  4.73897718e-02 -7.33566657e-02  5.22919357e-01]\n",
            " [-1.53359964e-01 -5.40179610e-01 -4.06891435e-01 -9.37043071e-01\n",
            "  -2.27882242e+00 -1.23697186e+00  5.63856244e-01 -1.36989832e+00\n",
            "  -6.82592988e-01 -1.48391545e+00  1.61625826e+00 -3.51966769e-01\n",
            "   8.12855482e-01  7.05759645e-01  1.58556652e+00 -8.38486254e-01\n",
            "   7.47208059e-01 -4.21323210e-01 -8.77941847e-01  1.35068440e+00\n",
            "   1.00736909e-01  1.25719905e-01  1.58278421e-01 -1.70515513e+00\n",
            "   1.00284964e-01  1.49099088e+00  2.90841293e+00  1.82270920e+00\n",
            "  -2.77768970e-01 -5.51692486e-01  1.37375677e+00  2.05268085e-01\n",
            "   6.25697374e-01  2.05362844e+00 -1.52366054e+00  7.55527079e-01\n",
            "   4.62852597e-01  7.31880784e-01  1.58326256e+00  2.18176627e+00\n",
            "  -9.19747591e-01  3.04486537e+00  6.28643751e-01  1.55757689e+00\n",
            "   7.99105167e-01 -1.02295232e+00 -4.93478060e-01  7.44665623e-01\n",
            "  -1.01384091e+00 -2.30880335e-01 -6.44427419e-01 -1.38608897e+00\n",
            "  -5.82872808e-01 -1.08889294e+00  1.14489543e+00  1.02748752e+00\n",
            "  -2.12490392e+00  1.36595035e+00  2.97649860e-01 -1.51979804e+00\n",
            "   6.09445386e-02  1.43451869e+00 -4.08879369e-01 -9.18281734e-01\n",
            "   9.13218081e-01  2.35200644e-01  5.49335815e-02 -1.06557894e+00\n",
            "  -9.71494913e-01 -6.41544104e-01  2.96794564e-01  8.38430762e-01\n",
            "   2.48536721e-01 -1.58761096e+00  1.94070375e+00 -9.87720907e-01\n",
            "  -1.06147027e+00 -5.95927119e-01 -1.36850789e-01  9.30582345e-01\n",
            "  -6.84497058e-02 -1.56926632e+00  6.82866335e-01 -3.46837878e-01\n",
            "   1.21881700e+00 -9.69519243e-02 -3.96502018e-01 -1.36298776e+00\n",
            "  -1.10478759e+00 -3.94686639e-01  5.52344501e-01 -2.99962431e-01\n",
            "  -2.47124100e+00  6.49322629e-01 -1.05223405e+00 -1.83353186e+00\n",
            "  -9.04538453e-01  1.56073618e+00  8.49141300e-01 -1.67442620e+00]\n",
            " [-2.06472707e+00  1.46429873e+00 -1.17742705e+00  1.02205598e+00\n",
            "   8.23291168e-02  6.85471833e-01  2.59675890e-01  5.25831617e-02\n",
            "  -1.17620611e+00  2.10781717e+00  2.09726334e+00 -1.49478614e+00\n",
            "   2.84203124e+00 -5.77786684e-01  2.67146492e+00 -4.54014897e-01\n",
            "   3.65922660e-01 -1.62212461e-01  4.76736724e-02  1.12449026e+00\n",
            "   1.19408262e+00 -7.87601918e-02 -9.90190864e-01 -2.43282676e+00\n",
            "   3.06138266e-02 -4.89971906e-01  1.49582088e+00  1.62432027e+00\n",
            "   1.51906693e+00 -3.54718000e-01  2.11083740e-01 -2.09513843e-01\n",
            "   2.34992552e+00  1.42355144e+00 -2.60248750e-01  2.86645025e-01\n",
            "   5.92181265e-01  1.07200289e+00  3.42731535e-01  2.29243231e+00\n",
            "  -6.42028034e-01  2.94670844e+00  3.84112805e-01  1.57727528e+00\n",
            "   3.75281930e-01 -1.53697395e+00  5.72744347e-02  1.45099893e-01\n",
            "  -1.45018446e+00 -1.24600732e+00  7.00087190e-01  6.13643467e-01\n",
            "   6.68383896e-01  1.93534946e+00  9.24082875e-01  1.27863061e+00\n",
            "  -6.32917404e-01 -1.34182438e-01  7.09026754e-01  1.34124219e-01\n",
            "   3.30398858e-01  1.17795670e+00 -1.59330153e+00 -1.11357546e+00\n",
            "   1.16627610e+00 -6.63340539e-02  6.02521360e-01 -2.80381590e-01\n",
            "  -1.21308267e-01 -1.02168679e+00 -2.45032683e-01  5.49350917e-01\n",
            "  -9.07678425e-01 -4.10212129e-01  1.47776103e+00 -4.05868471e-01\n",
            "  -1.13723516e+00  1.90453559e-01 -1.70517027e-01 -1.22420087e-01\n",
            "  -1.79940379e+00 -2.29364142e-01  6.65300548e-01  4.17033851e-01\n",
            "   5.44732474e-02  8.72509599e-01  2.54888892e-01 -1.44814813e+00\n",
            "  -2.41300082e+00 -9.08886671e-01 -2.68137842e-01  2.63687700e-01\n",
            "  -3.52275707e-02 -1.44501757e-02 -4.95612741e-01 -1.34367561e+00\n",
            "   2.39108801e-02  2.42026901e+00 -1.35583639e+00 -3.23103607e-01]\n",
            " [ 1.44850135e+00 -7.33823478e-01 -1.68170714e+00 -1.25549603e+00\n",
            "  -1.29074347e+00  3.06410909e-01 -1.34975299e-01 -6.11283444e-03\n",
            "   3.69971007e-01  1.63885641e+00  1.20890534e+00 -1.03958416e+00\n",
            "  -9.48939264e-01 -3.77306432e-01  2.24501085e+00 -1.86940718e+00\n",
            "   1.25880599e+00  8.14702213e-01  2.25328773e-01  2.02695489e+00\n",
            "   2.65928268e-01  9.42126274e-01  1.37716353e+00 -8.90317380e-01\n",
            "   1.25276220e+00  1.33313581e-01  4.27413464e-01  1.26198399e+00\n",
            "  -4.50307488e-01  2.50142485e-01 -1.11665852e-01  2.26896822e-01\n",
            "   8.27846825e-01  1.43341041e+00 -1.11691618e+00  1.77332115e+00\n",
            "   7.44989216e-01 -3.71541530e-01  1.90775847e+00  9.84743774e-01\n",
            "  -1.21548223e+00  2.50684571e+00  1.01615024e+00 -2.15713769e-01\n",
            "  -1.17013872e+00 -1.41171408e+00  1.05691224e-01  1.42718995e+00\n",
            "  -1.66185057e+00 -1.79106069e+00  2.36901700e-01  4.04287428e-01\n",
            "  -9.23234582e-01 -7.46156052e-02 -1.59636259e+00  2.04850006e+00\n",
            "  -1.21616530e+00  8.96634221e-01  1.07848978e+00  6.33555651e-01\n",
            "   3.44175279e-01 -2.98126668e-01  1.38384998e+00 -1.56641316e+00\n",
            "   2.01692510e+00 -6.97858930e-01  2.69974377e-02 -1.03386807e+00\n",
            "   3.28248203e-01 -4.97782558e-01 -9.58476588e-02 -1.46288073e+00\n",
            "  -8.68886173e-01 -2.07300782e+00  1.54625106e+00  5.67404509e-01\n",
            "  -1.79551947e+00 -6.76020861e-01  5.10554194e-01  1.51784360e-01\n",
            "  -6.73950851e-01  1.20356247e-01 -2.14086756e-01  9.34356809e-01\n",
            "   5.06093323e-01 -3.90106440e-01  1.48295522e+00  5.53037345e-01\n",
            "  -2.63946867e+00 -7.78107524e-01 -5.88462472e-01  9.00750041e-01\n",
            "  -1.42258060e+00  2.31469169e-01 -1.78808999e+00 -1.42882288e+00\n",
            "  -3.84326279e-01  9.13898885e-01 -1.71033239e+00 -5.73296487e-01]\n",
            " [ 5.42270718e-03  1.23178855e-01  4.73534524e-01 -3.73714641e-02\n",
            "  -8.19931552e-02  1.23708916e+00  8.55672136e-02  6.53726339e-01\n",
            "  -8.04103792e-01  3.70750815e-01  8.45243990e-01 -1.72200632e+00\n",
            "  -9.60618615e-01 -5.34405299e-02 -7.52197623e-01 -1.06977463e+00\n",
            "  -5.49918890e-01  7.38574505e-01 -2.76425093e-01 -6.54734552e-01\n",
            "  -3.92179579e-01 -3.54869753e-01 -7.12755382e-01 -6.03011250e-01\n",
            "   1.07725024e+00  5.26975572e-01 -6.89492285e-01  8.40234041e-01\n",
            "  -1.44785121e-01 -3.13275337e-01  9.03688908e-01 -6.20587349e-01\n",
            "  -1.60590100e+00 -4.31688428e-01  3.74624431e-01  1.08962655e+00\n",
            "   7.08198786e-01  5.90171039e-01 -3.66939336e-01 -1.25541830e+00\n",
            "  -1.80771291e+00  6.01344883e-01  1.00445652e+00  1.18448758e+00\n",
            "   7.22698689e-01  2.02653408e-01  1.60060883e+00  1.77949262e+00\n",
            "   3.03975081e+00  2.18889177e-01 -1.55429399e+00 -1.02829918e-01\n",
            "   2.19528008e+00  9.92667973e-01 -2.08086348e+00  8.44376087e-01\n",
            "   2.15682209e-01  1.26638031e+00  6.71681881e-01 -1.32065034e+00\n",
            "   5.05801141e-01 -2.44792223e-01 -5.80923319e-01  1.07969522e+00\n",
            "   8.29290748e-01 -3.57322916e-02 -4.03795630e-01 -1.37410605e+00\n",
            "   2.66996574e+00 -1.07133973e+00 -1.25296378e+00 -3.90341371e-01\n",
            "   2.01020822e-01  7.01156706e-02  1.18236411e+00 -1.24919641e+00\n",
            "  -1.30645826e-01 -1.07477045e+00  2.27072537e-01  8.58359158e-01\n",
            "   5.68589754e-02 -1.70693862e+00 -3.58987302e-01  3.58549505e-01\n",
            "  -6.10069046e-03  9.72192943e-01 -6.21437013e-01  8.97712171e-01\n",
            "   3.14062893e-01 -5.77719927e-01  1.07633710e+00  5.41679561e-02\n",
            "   2.40625307e-01  4.47415709e-01  3.95278066e-01 -7.21985757e-01\n",
            "   7.12453544e-01  8.12986046e-02 -1.77194452e+00 -1.90150309e+00]]\n",
            "9472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CA6HeD9jnKJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}